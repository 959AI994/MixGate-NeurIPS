# MixGate
Recent advances in circuit representation learning leverage Transformer-based frameworks to fuse multiple circuit views generated by EDA tools, yet how such multimodal fusion actually works remains unclear.In this work, we study this problem by analyzing the self-attention behavior of Transformer blocks operating on the tokenized circuits from different views. Our findings reveal that the model consistently attends to complementary implementations, enhancing its understanding of the same Boolean function.Building on this insight, we propose MixGate, a simple yet effective multiview learning strategy that augments existing Boolean circuit learning models with diverse circuit graphs.Extensive experiments show that MixGate significantly improves performance of circuit representation learning models, highlighting the value of cross-representation fusion in circuit learning.

# Pretrain MixGate model
## Pretrain MixGate-DeepGate2
bash run/pretrain_mixgate_deepgate2.sh

## Pretrain MixGate-PolarGate
bash run/pretrain_mixgate_polargate.sh

## Pretrain MixGate-HOGA
bash run/pretrain_mixgate_hoga.sh

## Pretrain MixGate-GCN
bash run/pretrain_mixgate_gcn.sh

## Pretrain MixGate-dg3
bash run/pretrain_mixgate_dg3.sh

# Pretrain graph encoder model
If you want to train an original encoderï¼Œyou can run these scripts.
## HOGA Encoder
bash run/train_hoga.sh
## PolarGate Encoder
bash run/train_polargate.sh
## GCN Encoder
bash run/train_gcn.sh
## DeepGate3 Encoder
bash run/train_dg3.sh

# Analyze attention
python3  analyze_attn.py


