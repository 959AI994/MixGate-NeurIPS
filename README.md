# Complementarity Matters: Multiview Representation Learning for Boolean Circuits

Official code repository for the paper: 
[**Complementarity Matters: Multiview Representation Learning for Boolean Circuits**]()

Authors: Anonymous author(s)

Abstract:
Recent advances in circuit representation learning leverage Transformer-based frameworks to fuse multiple circuit views generated by EDA tools, yet how such multimodal fusion actually works remains unclear.In this work, we study this problem by analyzing the self-attention behavior of Transformer blocks operating on the tokenized circuits from different views. Our findings reveal that the model consistently attends to complementary implementations, enhancing its understanding of the same Boolean function.Building on this insight, we propose MixGate, a simple yet effective multiview learning strategy that augments existing Boolean circuit learning models with diverse circuit graphs.Extensive experiments show that MixGate significantly improves performance of circuit representation learning models, highlighting the value of cross-representation fusion in circuit learning.

# Installation 

```bash
conda create -n mixgate python=3.8.20 -y
conda activate mixgate
pip install -r requirements.txt
```
# Pretrain MixGate model
## Pretrain MixGate-DeepGate2
```bash
bash run/pretrain_mixgate_deepgate2.sh
```
## Pretrain MixGate-PolarGate
```bash
bash run/pretrain_mixgate_polargate.sh
```
## Pretrain MixGate-HOGA
```bash
bash run/pretrain_mixgate_hoga.sh
```
## Pretrain MixGate-GCN
```bash
bash run/pretrain_mixgate_gcn.sh
```
## Pretrain MixGate-dg3
```bash
bash run/pretrain_mixgate_dg3.sh
```

# Pretrain graph encoder model
If you want to train an original encoderï¼Œyou can run these scripts.
## HOGA Encoder
```bash
bash run/train_hoga.sh
```
## PolarGate Encoder
```bash
bash run/train_polargate.sh
```
## GCN Encoder
```bash
bash run/train_gcn.sh
```
## DeepGate3 Encoder
```bash
bash run/train_dg3.sh
```
# Analyze attention
```bash
python3  analyze_attn.py
```

